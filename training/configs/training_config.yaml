# Simplified Training Configuration
# Minimal configuration for error-free PPO training

# Environment Configuration
environment:
  max_conversation_length: 20
  booking_agent_config:
    config_path: "agents/chat/config.yaml"
    db_path: "database/flights.db"
    server_path: "mcp-server/database_server.py"
  verifier_config:
    config_path: "agents/verifier/config.yaml"
  judge:
    config_path: "agents/judge/config.yaml"
    policy_set_path: "policies/booking_policies.yaml"
  auditor:
    config_path: "agents/auditor/config.yaml"
  process_reward:
    default_reward: 0.01
    armo:
      enabled: true
      model_path: ""  # Update with the local path to the ARMO reward checkpoint
      device: "auto"
      max_length: 1024
      max_history_turns: 6
      max_history_chars: 256
      score_scale: 1.0
      score_offset: 0.0
      clamp_min: -1.0
      clamp_max: 1.0
      heuristics:
        prompt_prefix: "Conversation transcript:"
        min_char_length: 20
        short_penalty: -0.1
        max_char_length: 400
        long_penalty: -0.05
        question_bonus: 0.05
        keywords:
          - flight
          - ticket
          - booking
          - reservation
        keyword_bonus: 0.05
        repetition_penalty: -0.2
        empty_penalty: -0.5

# PPO Training Configuration (Simplified)
ppo:
  # Basic training parameters
  learning_rate: 1e-4
  batch_size: 4
  mini_batch_size: 2

  # Reduced training scale for stability
  max_epochs: 5

  # PPO hyperparameters (conservative)
  ppo_epochs: 2
  gamma: 0.99
  lam: 0.95
  cliprange: 0.2
  vf_coef: 0.1
  ent_coef: 0.01
  max_grad_norm: 0.5  # Reduced for stability

  # Evaluation and saving
  save_freq: 2
  eval_freq: 1
  logging_steps: 5

  # Output directory
  output_dir: "./ppo_training_output"

# Evaluation Configuration (Minimal)
evaluation:
  num_eval_conversations: 3
  eval_objectives:
    - "I need to fly from New York to London"
    - "Looking for a ticket to Paris"
    - "Book a flight to Tokyo"

# Hardware Configuration (Conservative)
hardware:
  use_mixed_precision: false  # Disabled for stability
  gradient_checkpointing: false  # Disabled for simplicity

# GRPO Training Configuration (using TRL)
grpo:
  # Basic training parameters
  learning_rate: 5e-5  # Slightly higher for LoRA updates
  batch_size: 1
  group_size: 4  # Minimum for GRPO advantage computation

  # Training scale
  episodes: 20  # Total episodes to train

  # GRPO specific hyperparameters (TRL GRPOConfig)
  grpo_epochs: 4  # Number of optimization epochs per batch
  gamma: 0.99  # Discount factor
  lam: 0.95  # GAE lambda
  kl_coef: 0.05  # KL divergence coefficient (lower for TRL)

  # Optimization
  temperature: 0.7  # Temperature for generation
  max_grad_norm: 0.5  # Gradient clipping
  gradient_accumulation_steps: 4
  warmup_steps: 0
  max_steps: 1
  max_prompt_length: 128
  max_completion_length: 128

  # Experience replay
  use_buffer: false  # TRL GRPO handles batching internally

  # Evaluation and saving
  save_freq: 2
  eval_freq: 1
  logging_steps: 10
  save_steps: 500
  eval_steps: 500

  # Output directory
  output_dir: "./grpo_training_output"
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.05
  lora_target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

# Objective Generator Configuration
objective_generator:
  seed: 42

# Algorithm Selection
algorithm: "grpo"  # Options: "ppo", "grpo"
