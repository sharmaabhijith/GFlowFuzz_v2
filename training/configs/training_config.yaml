# Simplified Training Configuration
# Minimal configuration for error-free PPO training

# Environment Configuration
environment:
  max_conversation_length: 20
  booking_agent_config:
    config_path: "agents/chat/config.yaml"
    db_path: "database/flights.db"
    server_path: "mcp-server/database_server.py"
  verifier_config:
    config_path: "agents/verifier/config.yaml"
  auditor:
    config_path: "agents/auditor/config.yaml"

# PPO Training Configuration (Simplified)
ppo:
  # Basic training parameters
  learning_rate: 1e-4
  batch_size: 4
  mini_batch_size: 2

  # Reduced training scale for stability
  max_epochs: 5

  # PPO hyperparameters (conservative)
  ppo_epochs: 2
  gamma: 0.99
  lam: 0.95
  cliprange: 0.2
  vf_coef: 0.1
  ent_coef: 0.01
  max_grad_norm: 0.5  # Reduced for stability

  # Evaluation and saving
  save_freq: 2
  eval_freq: 1
  logging_steps: 5

  # Output directory
  output_dir: "./ppo_training_output"

# Evaluation Configuration (Minimal)
evaluation:
  num_eval_conversations: 3
  eval_objectives:
    - "I need to fly from New York to London"
    - "Looking for a ticket to Paris"
    - "Book a flight to Tokyo"

# Hardware Configuration (Conservative)
hardware:
  use_mixed_precision: false  # Disabled for stability
  gradient_checkpointing: false  # Disabled for simplicity

# GRPO Training Configuration (using TRL)
grpo:
  # Basic training parameters
  learning_rate: 5e-5  # Slightly higher for LoRA updates
  batch_size: 1
  group_size: 4  # Minimum for GRPO advantage computation

  # Training scale
  episodes: 20  # Total episodes to train

  # GRPO specific hyperparameters (TRL GRPOConfig)
  grpo_epochs: 4  # Number of optimization epochs per batch
  gamma: 0.99  # Discount factor
  lam: 0.95  # GAE lambda
  kl_coef: 0.05  # KL divergence coefficient (lower for TRL)

  # Optimization
  temperature: 0.7  # Temperature for generation
  max_grad_norm: 0.5  # Gradient clipping
  gradient_accumulation_steps: 4
  warmup_steps: 0
  max_steps: 1
  max_prompt_length: 128
  max_completion_length: 128

  # Experience replay
  use_buffer: false  # TRL GRPO handles batching internally

  # Evaluation and saving
  save_freq: 2
  eval_freq: 1
  logging_steps: 10
  save_steps: 500
  eval_steps: 500

  # Output directory
  output_dir: "./grpo_training_output"
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.05
  lora_target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

# Objective Generator Configuration
objective_generator:
  seed: 42
  complexity:
    min: 0
    max: 2
  weights:
    trip_type:
      one_way: 0.6
      round_trip: 0.4
    cabin:
      economy: 0.65
      business: 0.25
      first: 0.1
    direct:
      prefer: 0.6
      allow: 0.4
    passengers:
      "1": 0.6
      "2": 0.25
      "3": 0.1
      "4": 0.05
    extras:
      budget: 0.4
      airline: 0.2
      time_of_day: 0.2
      seat: 0.1
      bags: 0.1
      flexibility: 0.2

# Algorithm Selection
algorithm: "grpo"  # Options: "ppo", "grpo"
