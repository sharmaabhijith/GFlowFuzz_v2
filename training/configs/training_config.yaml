# Simplified Training Configuration
# Minimal configuration for error-free PPO training

# Environment Configuration
environment:
  max_conversation_length: 20
  booking_agent_config:
    config_path: "agents/chat/config.yaml"
    db_path: "database/flights.db"
    server_path: "mcp-server/database_server.py"
  verifier_config:
    config_path: "agents/verifier/config.yaml"
  judge:
    config_path: "agents/judge/config.yaml"
    policy_set_path: "policies/booking_policies.yaml"
  auditor:
    config_path: "agents/auditor/config.yaml"

# Evaluation Configuration (Minimal)
evaluation:
  num_eval_conversations: 3
  eval_objectives:
    - "I need to fly from New York to London"
    - "Looking for a ticket to Paris"
    - "Book a flight to Tokyo"

# Hardware Configuration (Conservative)
hardware:
  use_mixed_precision: false  # Disabled for stability
  gradient_checkpointing: false  # Disabled for simplicity

# GRPO Training Configuration (using TRL)
grpo:
  device: "mps"
  # Basic training parameters
  learning_rate: 5e-5  # Slightly higher for LoRA updates
  batch_size: 1
  group_size: 4  # Minimum for GRPO advantage computation

  # Training scale
  episodes: 20  # Total episodes to train

  # GRPO specific hyperparameters (TRL GRPOConfig)
  grpo_epochs: 4  # Number of optimization epochs per batch
  gamma: 0.99  # Discount factor
  lam: 0.95  # GAE lambda
  kl_coef: 0.05  # KL divergence coefficient (lower for TRL)

  # Optimization
  temperature: 0.7  # Temperature for generation
  max_grad_norm: 0.5  # Gradient clipping
  gradient_accumulation_steps: 4
  warmup_steps: 0
  max_steps: 1
  max_prompt_length: 128
  max_completion_length: 128

  # Experience replay
  use_buffer: false  # TRL GRPO handles batching internally

  # Evaluation and saving
  save_freq: 2
  eval_freq: 1
  logging_steps: 10
  save_steps: 500
  eval_steps: 500

  # Output directory
  output_dir: "./grpo_training_output"
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.05
  lora_target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

# GFlowNet Training Configuration
gflownet:
  device: "mps"
  learning_rate: 5e-5
  log_z_learning_rate: 5e-5
  weight_decay: 0.0
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  max_prompt_length: 256
  max_completion_length: 192
  min_reward: 1.0e-4
  reward_scale: 1.0
  reward_exponent: 1.0
  use_terminal_reward: true
  use_shaped_rewards: true
  shaped_reward_coef: 0.2
  entropy_coef: 0.0
  episodes: 20
  gamma: 0.99
  save_freq: 2
  output_dir: "./gflownet_training_output"
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.05
  lora_target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

# Objective Generator Configuration
objective_generator:
  seed: 42

# Algorithm Selection
algorithm: "gflownet"  # Options: "ppo", "grpo", "gflownet"
