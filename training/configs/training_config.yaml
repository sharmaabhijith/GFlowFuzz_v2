# Simplified Training Configuration
# Minimal configuration for error-free PPO training

# Environment Configuration
environment:
  max_conversation_length: 20
  booking_agent_config:
    config_path: "agents/chat/config.yaml"
    db_path: "database/flights.db"
    server_path: "mcp-server/database_server.py"
  verifier_config:
    config_path: "agents/verifier/config.yaml"

# Model Configuration
model:
  base_model: "Qwen/Qwen3-4B"
  tokenizer: "Qwen/Qwen3-4B"
  max_response_length: 30
  device: "auto"

# Auto User Agent Configuration
auto_user:
  model_name: "Qwen/Qwen3-4B"
  tokenizer_name: "Qwen/Qwen3-4B"
  temperature: 0.7
  max_tokens: 100
  top_p: 0.9
  repetition_penalty: 1.1

# PPO Training Configuration (Simplified)
ppo:
  # Basic training parameters
  learning_rate: 1e-4
  batch_size: 4
  mini_batch_size: 2

  # Reduced training scale for stability
  max_epochs: 5

  # PPO hyperparameters (conservative)
  ppo_epochs: 2
  gamma: 0.99
  lam: 0.95
  cliprange: 0.2
  vf_coef: 0.1
  ent_coef: 0.01
  max_grad_norm: 0.5  # Reduced for stability

  # Evaluation and saving
  save_freq: 2
  eval_freq: 1
  logging_steps: 5

  # Output directory
  output_dir: "./ppo_training_output"

# Reward System Configuration (Simplified)
reward_system:
  # Simple reward weights
  completion_weight: 1.0
  naturalness_weight: 0.2
  length_weight: 0.1

  # Length preferences
  min_conversation_length: 2
  max_conversation_length: 12
  optimal_length_range: [3, 11]

# Logging Configuration (Minimal)
logging:
  level: "INFO"
  save_conversations: false  # Disabled for simplicity
  save_verification_reports: false  # Disabled for simplicity

# Evaluation Configuration (Minimal)
evaluation:
  num_eval_conversations: 3
  eval_objectives:
    - "I need to fly from New York to London"
    - "Looking for a ticket to Paris"
    - "Book a flight to Tokyo"

# Hardware Configuration (Conservative)
hardware:
  use_mixed_precision: false  # Disabled for stability
  gradient_checkpointing: false  # Disabled for simplicity

# GRPO Training Configuration (using TRL)
grpo:
  # Basic training parameters
  learning_rate: 1e-5  # Lower than PPO for stability
  batch_size: 4
  group_size: 4  # Number of samples per group for relative comparison

  # Training scale
  max_epochs: 5
  max_conversations_per_epoch: 30

  # GRPO specific hyperparameters (TRL GRPOConfig)
  grpo_epochs: 4  # Number of optimization epochs per batch
  gamma: 0.99  # Discount factor
  lam: 0.95  # GAE lambda
  kl_coef: 0.05  # KL divergence coefficient (lower for TRL)

  # Optimization
  temperature: 0.7  # Temperature for generation
  max_grad_norm: 0.5  # Gradient clipping
  gradient_accumulation_steps: 1
  warmup_steps: 100

  # Experience replay
  use_buffer: false  # TRL GRPO handles batching internally

  # Evaluation and saving
  save_freq: 2
  eval_freq: 1
  logging_steps: 10
  save_steps: 500
  eval_steps: 500

  # Output directory
  output_dir: "./grpo_training_output"

# Algorithm Selection
algorithm: "grpo"  # Options: "ppo", "grpo"